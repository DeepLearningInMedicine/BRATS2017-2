{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup -- from CS 231N GANs notebook\n",
    "from __future__ import print_function, division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A bunch of utility functions\n",
    "\n",
    "def show_images(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def show_multimodal(image):\n",
    "    # images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    figdim = int(np.ceil(image.shape[0]/2))\n",
    "    # sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "    \n",
    "    dispInd = int(np.ceil(image.shape[3]/2))\n",
    "\n",
    "    fig = plt.figure(figsize=(figdim, figdim))\n",
    "    gs = gridspec.GridSpec(figdim, figdim)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i in range(image.shape[0]):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(image[i,:,:,dispInd])\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params():\n",
    "    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n",
    "    param_count = np.sum([np.prod(x.get_shape().as_list()) for x in tf.global_variables()])\n",
    "    return param_count\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Open \n",
    "path = '../HGG/Brats17_2013_2_1/Brats17_2013_2_1_flair.nii'\n",
    "flair_img = nib.load(path).get_data()\n",
    "\n",
    "path = '../HGG/Brats17_2013_2_1/Brats17_2013_2_1_t1.nii'\n",
    "t1_img = nib.load(path).get_data()\n",
    "\n",
    "path = '../HGG/Brats17_2013_2_1/Brats17_2013_2_1_t2.nii'\n",
    "t2_img = nib.load(path).get_data()\n",
    "\n",
    "path = '../HGG/Brats17_2013_2_1/Brats17_2013_2_1_t1ce.nii'\n",
    "t1ce_img = nib.load(path).get_data()\n",
    "\n",
    "path = '../HGG/Brats17_2013_2_1/Brats17_2013_2_1_seg.nii'\n",
    "seg_img = nib.load(path).get_data()\n",
    "\n",
    "# img = np.zeros((4, 240, 240, 155))\n",
    "# img[0,:,:,:] = flair_img\n",
    "# img[1,:,:,:] = t1_img\n",
    "# img[2,:,:,:] = t2_img\n",
    "# img[3,:,:,:] = t1ce_img\n",
    "\n",
    "# show_multimodal(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-297ba8c71399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mHGGnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflair_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHGGnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHGGsegnp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msavez_compressed\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                 \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type)\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0mCRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCRC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcmpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                     \u001b[0mcompress_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compress files into numpy compressed form --- maybe use later\n",
    "i = 0\n",
    "for root, dirs, files in os.walk('../HGG/'):\n",
    "    if len(files) > 1:\n",
    "        HGGnp = np.zeros((4,240,240,155))\n",
    "\n",
    "        flair_img = nib.load(root + '/' + files[-5]).get_data()\n",
    "        HGGnp[0,:,:,:] = flair_img\n",
    "        \n",
    "        seg = nib.load(root + '/' + files[-4]).get_data()\n",
    "        HGGsegnp = seg\n",
    "        \n",
    "        t1_img = nib.load(root + '/' + files[-3]).get_data()\n",
    "        HGGnp[1,:,:,:] = flair_img\n",
    "        \n",
    "        t1ce_img = nib.load(root + '/' + files[-2]).get_data()\n",
    "        HGGnp[2,:,:,:] = flair_img\n",
    "        \n",
    "        t2_img = nib.load(root + '/' + files[-1]).get_data()\n",
    "        HGGnp[3,:,:,:] = flair_img\n",
    "        \n",
    "        np.savez_compressed(root + '/' + files[-1][:-7], img = HGGnp, seg = HGGsegnp)\n",
    "        \n",
    "        i +=1\n",
    "        print(i)\n",
    "\n",
    "\n",
    "# HGG = tf.Variable(tf.stack(HGGnp), name=\"HGG\", trainable=False)\n",
    "\n",
    "i = 0\n",
    "for root, dirs, files in os.walk('../LGG/'):\n",
    "    if len(files) > 1:\n",
    "        LGGnp = np.zeros((4,240,240,155))\n",
    "\n",
    "        flair_img = nib.load(root + '/' + files[-5]).get_data()\n",
    "        LGGnp[0,:,:,:] = flair_img\n",
    "        \n",
    "        seg = nib.load(root + '/' + files[-4]).get_data()\n",
    "        LGGsegnp = seg\n",
    "        \n",
    "        t1_img = nib.load(root + '/' + files[-3]).get_data()\n",
    "        LGGnp[1,:,:,:] = flair_img\n",
    "        \n",
    "        t1ce_img = nib.load(root + '/' + files[-2]).get_data()\n",
    "        LGGnp[2,:,:,:] = flair_img\n",
    "        \n",
    "        t2_img = nib.load(root + '/' + files[-1]).get_data()\n",
    "        LGGnp[3,:,:,:] = flair_img\n",
    "        \n",
    "        np.savez_compressed(root + '/' + files[-1][:-7], img = LGGnp, seg = LGGsegnp)\n",
    "        \n",
    "        i +=1\n",
    "        print(i)\n",
    "\n",
    "# LGG = tf.Variable(tf.zeros([75,4,240,240,155]), name=\"LGG\", trainable=False)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LGGnp = np.zeros((75,4,240,240,155))\n",
    "i = 0\n",
    "for root, dirs, files in os.walk('../LGG/'):\n",
    "    if len(files) > 1:\n",
    "        LGGnp = np.zeros((4,240,240,155))\n",
    "\n",
    "        flair_img = nib.load(root + '/' + files[-5]).get_data()\n",
    "        LGGnp[0,:,:,:] = flair_img\n",
    "        \n",
    "        seg = nib.load(root + '/' + files[-4]).get_data()\n",
    "        LGGsegnp = seg\n",
    "        \n",
    "        t1_img = nib.load(root + '/' + files[-3]).get_data()\n",
    "        LGGnp[1,:,:,:] = flair_img\n",
    "        \n",
    "        t1ce_img = nib.load(root + '/' + files[-2]).get_data()\n",
    "        LGGnp[2,:,:,:] = flair_img\n",
    "        \n",
    "        t2_img = nib.load(root + '/' + files[-1]).get_data()\n",
    "        LGGnp[3,:,:,:] = flair_img\n",
    "        \n",
    "        np.savez_compressed(root + '/' + files[-1][:-7], img = LGGnp, seg = LGGsegnp)\n",
    "        \n",
    "        i +=1\n",
    "        print(i)\n",
    "        \n",
    "LGG = tf.Variable(tf.stack(LGGnp), name=\"LGG\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data feeding function\n",
    "\n",
    "def BRATSdatafeed()\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dice Loss\n",
    "def DICEscore(truth, estimate)\n",
    "    #\n",
    "    # truth, estimate = tf tensors of size N x voxels*channels\n",
    "    # Computes average Dice scores\n",
    "    #\n",
    "    # loss = scalar tensor containing Dice loss \n",
    "    # \n",
    "    \n",
    "    loss = 2*tf.reduce_mean(tf.divide(tf.reduce_sum(tf.multiply(truth,estimate),axis=1),\n",
    "                       (tf.reduce_sum(truth,axis=1) + tf.reduce_sum(estimate,axis=1))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def brainconvnet(x):\n",
    "    \"\"\"Build computational graph \n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, #voxels * 4]\n",
    "    IMPORTANT: x must be a channels-first tensor (before resizing)\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, #voxels * 5]\n",
    "    (Graph calculates 5th order tensor: channel 1 for image, channel 2 for class, others for voxels)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"convNet\"):\n",
    "        input_size = x.get_shape().as_list()\n",
    "        x_reshape = tf.reshape(x, [-1,240,240,155,4])\n",
    "        conv1 = tf.layers.conv3d(inputs=x_reshape, filters=8, \n",
    "                                 kernel_size=[11, 11, 11],padding=\"same\", activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling3d(inputs = conv1, pool_size = (2,2,2),\n",
    "                                        strides = (2,2,2), padding='valid',name=None)\n",
    "        \n",
    "        conv2 = tf.layers.conv3d(inputs=pool1, filters=8, \n",
    "                                 kernel_size=[5, 5, 5],padding=\"same\", activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling3d(inputs = conv2, pool_size = (2,2,2),\n",
    "                                        strides = (2,2,2), padding='valid',name=None)\n",
    "        \n",
    "        conv3 = tf.layers.conv3d(inputs=pool2, filters=32, \n",
    "                                 kernel_size=[3, 3, 3],padding=\"same\", activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling3d(inputs = conv3, pool_size = (2,2,2),\n",
    "                                        strides = (2,2,2), padding='valid',name=None)\n",
    "        \n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=128, \n",
    "                                 kernel_size=[3, 3, 3],padding=\"same\", activation=tf.nn.relu)\n",
    "        \n",
    "#         return conv4\n",
    "            \n",
    "    with tf.variable_scope(\"deconvNet\"):\n",
    "        W3 = tf.Variable(tf.truncated_normal([3, 3, 3, 32, 128], stddev=0.1))\n",
    "        deconv3 = tf.nn.conv3d_transpose(conv4, filter = W3, output_shape = [input_size[0],60, 60, 36, 32], \n",
    "                                         strides = [1,1,1,1,1])\n",
    "        b3 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "        relu3 = tf.nn.relu(deconv3 + b3)\n",
    "        \n",
    "        W2 = tf.Variable(tf.truncated_normal([3, 3, 3, 8, 32], stddev=0.1))\n",
    "        deconv2 = tf.nn.conv3d_transpose(relu3, filter = W2, output_shape = [input_size[0],120, 120, 72, 8], \n",
    "                                         strides = [1,1,1,1,1])\n",
    "        b2 = tf.Variable(tf.constant(0.1, shape=[8]))\n",
    "        relu2 = tf.nn.relu(deconv2 + b2)\n",
    "        \n",
    "        W1 = tf.Variable(tf.truncated_normal([3, 3, 3, 5, 8], stddev=0.1))\n",
    "        deconv1 = tf.nn.conv3d_transpose(relu2, filter = W1, output_shape = [input_size[0],240, 240, 155, 5], \n",
    "                                         strides = [1,1,1,1,1])\n",
    "        b1 = tf.Variable(tf.constant(0.1, shape=[5]))\n",
    "        img = tf.reshape(deconv1 + b1,[-1,5*240*240*155])\n",
    "        \n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286901\n"
     ]
    }
   ],
   "source": [
    "# Get number of network parameters\n",
    "def paramcount():\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        y = brainconvnet(tf.ones((2, 240*240*155*4)))\n",
    "        cur_count = count_params()\n",
    "        return cur_count\n",
    "\n",
    "print(paramcount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 128\n",
    "# our noise dimension\n",
    "noise_dim = 96\n",
    "\n",
    "# placeholders for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "z = sample_noise(batch_size, noise_dim)\n",
    "# generated images\n",
    "G_sample = generator(z)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    #scale images to be -1 to 1\n",
    "    logits_real = discriminator(preprocess_img(x))\n",
    "    # Re-use discriminator weights on new inputs\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'generator') \n",
    "\n",
    "D_solver,G_solver = get_solvers()\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "D_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'discriminator')\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'generator')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
